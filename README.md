# Glossy_llm
ğŸš€ Fine-Tuning Phi-2 with 4-bit DoRA PEFT
ğŸ“Œ Overview
This project fine-tunes Microsoft Phi-2 using 4-bit LoRA and DoRA (Decoupled LoRA) for efficient training on low-cost hardware. The goal is to optimize text-to-gloss generation while reducing memory usage.

âš™ï¸ Dependencies
Ensure the following packages are installed:

Python 3.8+

PyTorch (with CUDA support for GPU)

Hugging Face Libraries: transformers, datasets

Quantization & PEFT: bitsandbytes, peft, dora

Other Utilities: scikit-learn, pandas, numpy

ğŸ“‚ Dataset
The dataset consists of text-gloss pairs in CSV format. Ensure data is preprocessed and tokenized before training.

ğŸš€ Training Process
Load Phi-2 with 4-bit LoRA & DoRA

Tokenize and split the dataset

Configure training parameters

Train using Hugging Face Trainer with early stopping

ğŸ’¾ Model Usage
After training, the model can be saved locally or pushed to Hugging Face Hub for deployment.

ğŸ“œ License
This project is licensed under the MIT License.

ğŸ“¬ Contact
For inquiries, reach out via GitHub or Email.

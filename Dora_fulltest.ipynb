{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11194131,"sourceType":"datasetVersion","datasetId":6988458}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers torch datasets accelerate peft bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:46:00.672273Z","iopub.execute_input":"2025-03-28T11:46:00.672589Z","iopub.status.idle":"2025-03-28T11:46:04.109405Z","shell.execute_reply.started":"2025-03-28T11:46:00.672564Z","shell.execute_reply":"2025-03-28T11:46:04.107749Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:46:04.111586Z","iopub.execute_input":"2025-03-28T11:46:04.111985Z","iopub.status.idle":"2025-03-28T11:46:04.117205Z","shell.execute_reply.started":"2025-03-28T11:46:04.111943Z","shell.execute_reply":"2025-03-28T11:46:04.116269Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Load and clean dataset\nfile_path = \"/kaggle/input/gloss-textpairs/words and code - Sheet2.csv\"\ndata = pd.read_csv(file_path)\ndata.rename(columns={'ZZZ': 'Text'}, inplace=True)\ndata_tmp = data[['Text', 'Gloss']].dropna()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:46:04.118723Z","iopub.execute_input":"2025-03-28T11:46:04.118960Z","iopub.status.idle":"2025-03-28T11:46:04.165051Z","shell.execute_reply.started":"2025-03-28T11:46:04.118938Z","shell.execute_reply":"2025-03-28T11:46:04.164422Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# Convert columns to string format\ndata_tmp[\"Text\"] = data_tmp[\"Text\"].astype(str)\ndata_tmp[\"Gloss\"] = data_tmp[\"Gloss\"].astype(str)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:46:04.166024Z","iopub.execute_input":"2025-03-28T11:46:04.166297Z","iopub.status.idle":"2025-03-28T11:46:04.171060Z","shell.execute_reply.started":"2025-03-28T11:46:04.166247Z","shell.execute_reply":"2025-03-28T11:46:04.170070Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Load Phi-2 model with 4-bit LoRA\nmodel_name = \"microsoft/phi-2\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    load_in_4bit=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token  # Set padding token\n\n# Define prompt template for instruction fine-tuning\nprompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nConvert the following English sentence into ASL gloss.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    texts = []\n    for text, gloss in zip(examples[\"Text\"], examples[\"Gloss\"]):\n        formatted_text = prompt_template.format(text, gloss) + EOS_TOKEN\n        texts.append(formatted_text)\n    return {\"text\": texts}\n\n# Convert to Hugging Face Dataset and format text\ndataset = Dataset.from_pandas(data_tmp)\ndataset = dataset.map(formatting_prompts_func, batched=True)\n\n# Tokenization function with labels\ndef tokenize_function(examples):\n    model_inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()  # Ensure labels exist\n    return model_inputs\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\n# Convert dataset to Pandas for splitting\ndf = tokenized_dataset.to_pandas()\n\ndf = df.loc[:, ~df.columns.duplicated()]  # Remove duplicate columns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:46:04.172058Z","iopub.execute_input":"2025-03-28T11:46:04.172374Z","iopub.status.idle":"2025-03-28T11:46:08.635714Z","shell.execute_reply.started":"2025-03-28T11:46:04.172319Z","shell.execute_reply":"2025-03-28T11:46:08.635064Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07ae0cbba62c4bef9623e54c97174f41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5043 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10af4de387af4d98b1071e6d1020ec32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5043 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127e70324b6c4e318b1fd4a679680a90"}},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"# Train-test split (80% train, 20% validation)\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Convert back to Hugging Face Dataset format\ntrain_dataset = Dataset.from_pandas(train_df, preserve_index=False)\nval_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n\n# Create dataset dictionary\ndataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\nprint(f\"Train size: {len(dataset_dict['train'])}, Validation size: {len(dataset_dict['validation'])}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:46:08.636507Z","iopub.execute_input":"2025-03-28T11:46:08.636765Z","iopub.status.idle":"2025-03-28T11:46:08.753243Z","shell.execute_reply.started":"2025-03-28T11:46:08.636741Z","shell.execute_reply":"2025-03-28T11:46:08.752320Z"}},"outputs":[{"name":"stdout","text":"Train size: 4034, Validation size: 1009\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# LoRA configuration\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    use_dora=True\n)\n\n# Prepare model for training with 4-bit LoRA\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:46:08.754323Z","iopub.execute_input":"2025-03-28T11:46:08.754557Z","iopub.status.idle":"2025-03-28T11:46:09.145895Z","shell.execute_reply.started":"2025-03-28T11:46:08.754533Z","shell.execute_reply":"2025-03-28T11:46:09.144996Z"}},"outputs":[{"name":"stdout","text":"trainable params: 9,748,480 || all params: 2,789,432,320 || trainable%: 0.3495\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# Training arguments with reduced time\ntraining_args = TrainingArguments(\n    output_dir=\"./phi2_lora_finetune\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,  # Reduce training time\n    evaluation_strategy=\"steps\",\n    eval_steps=50,\n    save_strategy=\"steps\",  # Changed to 'steps' to match evaluation_strategy\n    save_steps=50,        # Save every 50 steps\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    learning_rate=5e-5,  # Slightly lower for stability\n    weight_decay=0.01,\n    num_train_epochs=1,  # Reduce epochs to speed up training\n    save_total_limit=1,  # Save only the best checkpoint\n    metric_for_best_model=\"loss\",\n    greater_is_better=False,\n    load_best_model_at_end=True,\n    bf16=True,  # More stable for 4-bit LoRA training\n    report_to=\"none\"\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:46:09.147758Z","iopub.execute_input":"2025-03-28T11:46:09.147992Z","iopub.status.idle":"2025-03-28T11:46:09.175099Z","shell.execute_reply.started":"2025-03-28T11:46:09.147971Z","shell.execute_reply":"2025-03-28T11:46:09.174264Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# Trainer setup with early stopping\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Reduce early stopping patience\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:46:09.176582Z","iopub.execute_input":"2025-03-28T11:46:09.176919Z","iopub.status.idle":"2025-03-28T11:46:09.209318Z","shell.execute_reply.started":"2025-03-28T11:46:09.176885Z","shell.execute_reply":"2025-03-28T11:46:09.208488Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-61-32aa14be8e8f>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"#Train the model\n# trainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:46:09.210384Z","iopub.execute_input":"2025-03-28T11:46:09.210612Z","iopub.status.idle":"2025-03-28T11:46:09.214203Z","shell.execute_reply.started":"2025-03-28T11:46:09.210590Z","shell.execute_reply":"2025-03-28T11:46:09.213309Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Define the saved model path\nsave_path = \"/kaggle/working/phi2_lora_finetune/checkpoint-250\"\n\n\n# Load the model in half-precision (float16) and send it to GPU if available\nmodel = AutoModelForCausalLM.from_pretrained(\n    save_path,\n    torch_dtype=torch.float16,  # Reduce memory usage\n    device_map=\"auto\"  # Automatically place on GPU if available\n)\ntokenizer = AutoTokenizer.from_pretrained(save_path)\n\nprint(\"âœ… Model(dora) loaded successfully with optimized memory usage!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:47:03.404739Z","iopub.execute_input":"2025-03-28T11:47:03.405067Z","iopub.status.idle":"2025-03-28T11:47:06.915393Z","shell.execute_reply.started":"2025-03-28T11:47:03.405041Z","shell.execute_reply":"2025-03-28T11:47:06.914469Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f2551d9cd243658034afba78fba991"}},"metadata":{}},{"name":"stdout","text":"âœ… Model(dora) loaded successfully with optimized memory usage!\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"**Let's evaluate it**\n","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Ensure tokenizer is set up correctly\ntokenizer.padding_side = \"left\"  # Fix padding warning\ntokenizer.pad_token = tokenizer.eos_token  # Ensure pad token is set\n\ndef generate_gloss_batch(text_list, max_new_tokens=50, batch_size=4):\n    gloss_outputs = []\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)  # Move model to GPU if available\n\n    for i in range(0, len(text_list), batch_size):  # Process in mini-batches\n        batch_texts = text_list[i:i+batch_size]\n\n        # Format prompts\n        prompts = [f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n        ### Instruction:\n        Convert the following English sentence into ASL gloss.\n\n        ### Input:\n        {text}\n\n        ### Response:\n\"\"\" for text in batch_texts]\n\n        # Tokenization with explicit padding and attention mask\n        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n        input_ids = inputs.input_ids.to(device)\n        attention_mask = inputs.attention_mask.to(device)\n\n        # Generate output with max_new_tokens instead of max_length\n        with torch.inference_mode():\n            output_ids = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens)\n\n        # Decode outputs\n        generated_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n\n        # Extract gloss from each response\n        glosses = [gen.split(\"### Response:\")[-1].strip() for gen in generated_texts]\n        gloss_outputs.extend(glosses)\n\n        # Free memory\n        del inputs, input_ids, attention_mask, output_ids\n        torch.cuda.empty_cache()  # Clear GPU memory\n\n    return gloss_outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:47:14.300404Z","iopub.execute_input":"2025-03-28T11:47:14.300715Z","iopub.status.idle":"2025-03-28T11:47:14.307962Z","shell.execute_reply.started":"2025-03-28T11:47:14.300687Z","shell.execute_reply":"2025-03-28T11:47:14.306559Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"\n# Example test\ntext_samples = [  \"The magician performed amazing tricks\",\n    \"The dentist checked my teeth carefully\",\n    \"He left school\",\n    \"The dog barked loudly\",\n    \"She took pictures today\"\n\n]\n\n\ngloss_outputs = generate_gloss_batch(text_samples)\nfor i, gloss in enumerate(gloss_outputs):\n    print(f\"Original: {text_samples[i]}\")\n    print(f\"Generated ASL Gloss: {gloss}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:47:18.121521Z","iopub.execute_input":"2025-03-28T11:47:18.121970Z","iopub.status.idle":"2025-03-28T11:47:24.326637Z","shell.execute_reply.started":"2025-03-28T11:47:18.121929Z","shell.execute_reply":"2025-03-28T11:47:24.325737Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Original: The magician performed amazing tricks\nGenerated ASL Gloss: MAGICIAN TRICKS AMAZING PERFORM\n\nOriginal: The dentist checked my teeth carefully\nGenerated ASL Gloss: TEETH DENTIST CHECK CAREFULLY\n\nOriginal: He left school\nGenerated ASL Gloss: HIM SCHOOL LEFT\n\nOriginal: The dog barked loudly\nGenerated ASL Gloss: DOG BARK LOUDLY\n\nOriginal: She took pictures today\nGenerated ASL Gloss: HER TODAY PHOTO TAKEN\n\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}